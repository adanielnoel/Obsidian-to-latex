@article{Feline1999,
abstract = {We evaluate 179 classifiers arising from 17 families (discriminant analysis, Bayesian, neural networks, support vector machines, decision trees, rule-based classifi ers, boosting, bagging, stacking, random forests and other ensembles, generalized linear models, nearestneighbors, partial least squares and principal component regression, logistic and multinomial regression, multiple adaptive regression splines and other methods), implemented in Weka, R (with and without the caret package), C and Matlab, including all the relevant classifiers available today. We use 121 data sets, which represent the whole UCI data base (excluding the large-scale problems) and other own real problems, in order to achieve significant conclusions about the classifier behavior, not dependent on the data set collection. The classifiers most likely to be the bests are the random forest (RF) versions, the best of which (implemented in R and accessed via caret) achieves 94.1{\%} of the maximum accuracy overcoming 90{\%} in the 84.3{\%} of the data sets. However, the difference is not statistically significant with the second best, the SVM with Gaussian kernel implemented in C using LibSVM, which achieves 92.3{\%} of the maximum accuracy. A few models are clearly better than the remaining ones: random forest, SVM with Gaussian and polynomial kernels, extreme learning machine with Gaussian kernel, C5.0 and avNNet (a committee of multi-layer perceptrons implemented in R with the caret package). The random forest is clearly the best family of classifiers (3 out of 5 bests classi ers are RF), followed by SVM (4 classifiers in the top-10), neural networks and boosting ensembles (5 and 3 members in the top-20, respectively).},
author = {Fern{\'{a}}ndez-Delgado, Manuel and Cernadas, Eva and Barro, Sen{\'{e}}n and Amorim, Dinani},
doi = {10.1117/1.JRS.11.015020},
file = {:Users/alejandrodanielnoel1/Documents/Mendeley Desktop/Fern{\'{a}}ndez-Delgado et al.{\_}2014{\_}Do we need hundreds of classifiers to solve real world classification problems.pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Bayesian classifiers,Classification,Decision trees,Discriminant analysis,Ensembles,Generalized linear models,Logistic and multinomial regression,Multiple adaptive regression splines,Nearest-neighbors,Neural networks,Partial least squares and principal component regr,Random forest,Rule-based classifiers,Support vector machine,UCI data base},
pages = {3133--3181},
title = {{Do we need hundreds of classifiers to solve real world classification problems?}},
volume = {15},
year = {2014}
}
@article{Smooth2010,
abstract = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp(mu) performance criteria, for arbitrary finite input environment measures mu, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives.},
author = {Hornik, Kurt},
doi = {10.1016/0893-6080(91)90009-T},
file = {:Users/alejandrodanielnoel1/Documents/Mendeley Desktop/Hornik{\_}1991{\_}Approximation capabilities of multilayer feedforward networks.pdf:pdf},
issn = {08936080},
journal = {Neural Networks},
keywords = {Lp(mu) approximation,Sobolev spaces,activation function,input environment measure,multilayer feedforward networks,smooth approximation,uniform approximation,universal approximation capabilities},
number = {2},
pages = {251--257},
title = {{Approximation capabilities of multilayer feedforward networks}},
url = {https://linkinghub.elsevier.com/retrieve/pii/089360809190009T},
volume = {4},
year = {1991}
}